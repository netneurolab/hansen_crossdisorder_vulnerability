"""
This script compares regional atrophy to mean neighbour atrophy
and then looks for disease epicentres. Analyses here correspond
to Figure 3 of the manuscript.
"""


import numpy as np
import pandas as pd
from scipy.stats import zscore, pearsonr
import matplotlib.pyplot as plt
from netneurotools import stats, datasets, plotting, utils
from neuromaps.nulls import burt2020, burt2018
from neuromaps import images
from matplotlib.colors import ListedColormap


def get_neighbour_deformation(node_deformation, sc, fc=None):
    '''
    compute mean neighbour deformation of each node

    Inputs:

    node_deformation = n x 1 matrix of node deformations
    sc = n x n structural connectivity matrix (binary or weighted)
    fc = n x n functional connectivity matrix (optional)

    Output:

        neighbour_deformation = n x 1 matrix of mean neighbour deformations
    '''

    nnodes = len(node_deformation)
    neighbour_deformation = np.zeros((nnodes,))

    FC = np.ones((nnodes, nnodes))

    if fc is not None:
        FC = fc

    for i in range(nnodes):
        neighbour_deformation[i] = np.mean(node_deformation[sc[i, :] != 0]
                                           * sc[sc[i, :] != 0, i]
                                           * FC[sc[i, :] != 0, i])

    return neighbour_deformation


def get_epicentre_likelihood(node_deformation, neighbour_deformation,
                             spins=None, nspins=None):
    '''
    epicenter likelihood ordered same as node and neighbour deformation

    node_deformation: (n, ) array_like
        Node deformation ordered such that a small value represents more atrophy.
    neighbour_deformation: (n, ) array_like
        Neighbour deformation ordered such that a small value represents more atrophy.

    Returns
    -------
    epicentre_likelihood: (n, ) ndarray
        Epicentre likelihood, where a larger value represents greater likelihood.
    epicentre_pvals: (n, ) ndarray
        Spin-test one-tailed p-value of epicentre likelihood.
    
    '''
    a = np.argsort(node_deformation)[::-1]
    b = np.zeros((len(a), ))
    for k in range(len(a)):
        b[a[k]] = k
    aa = np.argsort(neighbour_deformation)[::-1]
    bb = np.zeros((len(aa), ))
    for k in range(len(aa)):
        bb[aa[k]] = k

    epicentre_likelihood = np.mean(np.stack((b, bb), axis=1), axis=1)
    epicentre_pvals = np.ones(epicentre_likelihood.shape)

    if spins is not None:
        null = np.zeros((len(node_deformation), nspins))
        for k in range(nspins):
            a = np.argsort(node_deformation[spins[:, k]])[::-1]
            b = np.zeros((len(a), ))
            for j in range(len(a)):
                b[a[j]] = j
            null[:, k] = np.mean(np.stack((b, bb), axis=1), axis=1)
        epicentre_pvals = (1 + np.sum(null > np.expand_dims(epicentre_likelihood, 1),
                                      axis=1)) / (nspins + 1)

    return epicentre_likelihood, epicentre_pvals


def corr_spin(x, y, spins, nspins):
    rho, _ = pearsonr(x, y)
    null = np.zeros((nspins,))

    # null correlation
    for i in range(nspins):
        null[i], _ = pearsonr(x, y[spins[:, i]])

    pval = (1 + sum(abs((null - np.mean(null))) >
                    abs((rho - np.mean(null))))) / (nspins + 1)
    return rho, pval


# set path to directory
path = "C:/Users/justi/OneDrive - McGill University/MisicLab/proj_biology_disease/github/hansen_crossdisorder_vulnerability/"

# dataset 
dataset = 'lausanne'

"""
load
"""
# disorder maps
ct = np.genfromtxt(path+'data/enigma_ct.csv', delimiter=',')
ct = zscore(ct)
disorders = np.load(path+'data/disorders.npy')

# dominance analysis result
dominance = {}
dominance['biological'] = np.load(path+'results/' + dataset + '/dominance_biological.npy')
dominance['connectivity'] = np.load(path+'results/' + dataset + '/dominance_connectivity.npy')

# sc and fc
sc = np.load(path+'data/' + dataset + '/sc_weighted.npy')
fc = np.load(path+'data/' + dataset + '/fc_weighted.npy')

# spin samples
cammoun = datasets.fetch_cammoun2012()
info = pd.read_csv(cammoun['info'])
cortex = info.query('scale == "scale033" & structure == "cortex"')['id']
cortex = np.array(cortex) - 1  # python indexing
hemiid = np.array(info.query('scale == "scale033"')['hemisphere'])
hemiid = hemiid == 'R'
coords = utils.get_centroids(cammoun['scale033'], image_space=True)
nspins = 10000
spins = stats.gen_spinsamples(coords[cortex, :], hemiid[cortex],
                              n_rotate=nspins, seed=1234)

# colourmap
cmap = np.genfromtxt(path+'data/colourmap.csv', delimiter=',')
cmap_seq = ListedColormap(cmap[128:, :])

"""
correlation regional atrophy to neighbour atrophy
"""

# plot node-neighbour correlations for each disorder
plt.ion()
# sc-weighted only
fig1, axs1 = plt.subplots(4, 4, figsize=(7, 7))
fig1.subplots_adjust(hspace=1)
axs1 = axs1.ravel()

# sc- and fc-weighted
fig2, axs2 = plt.subplots(4, 4, figsize=(7, 7))
fig2.subplots_adjust(hspace=1)
axs2 = axs2.ravel()

nncorr = {}
pspin = {}

nncorr['sc-only'] = np.zeros((len(disorders), ))  # node-neighbour correlation
pspin['sc-only'] = np.zeros((len(disorders), ))
nncorr['fc-also'] = np.zeros((len(disorders), ))
pspin['fc-also'] = np.zeros((len(disorders), ))
for i in range(len(disorders)):
    print(i)
    neighbour_deformation_sc = get_neighbour_deformation(ct[:, i], sc, fc=None)
    neighbour_deformation_fc = get_neighbour_deformation(ct[:, i], sc, fc=fc)

    axs1[i].scatter(ct[:, i], neighbour_deformation_sc, s=5)
    axs1[i].set_title(disorders[i])
    axs1[i].set_xlim([-2.8, 2.8])
    axs1[i].set_ylim([-0.7, 0.7])

    axs2[i].scatter(ct[:, i], neighbour_deformation_fc, s=5)
    axs2[i].set_title(disorders[i])
    axs2[i].set_xlim([-2.8, 2.8])
    axs2[i].set_ylim([-0.4, 0.4])
    
    nncorr['sc-only'][i], pspin['sc-only'][i] = corr_spin(ct[:, i],
                                                          neighbour_deformation_sc,
                                                          spins, nspins)
    nncorr['fc-also'][i], pspin['fc-also'][i] = corr_spin(ct[:, i],
                                                          neighbour_deformation_fc,
                                                          spins, nspins)

fig1.savefig(path+'figures/' + dataset + '/scatter_node_neighbour_sc.eps', format='eps')
fig2.savefig(path+'figures/' + dataset + '/scatter_node_neighbour_fc.eps', format='eps')
np.savez(path+'results/' + dataset + '/node_neighbour_rho_pvals.npz',
         rho_sc=nncorr['sc-only'], pval_sc=pspin['sc-only'],
         rho_fc=nncorr['fc-also'], pval_fc=pspin['fc-also'])

"""
Network-spreading atrophy patterning is related to r-squared
"""
bio_rsq = np.sum(dominance["biological"], axis=1)
conn_rsq = np.sum(dominance["connectivity"], axis=1)

for k in nncorr.keys():
    plt.ion()
    fig, ax = plt.subplots()
    ax.scatter(bio_rsq, nncorr[k], c=(pspin[k] < 0.05).astype(int))
    for i in range(len(bio_rsq)):
        ax.text(bio_rsq[i] + 0.01, nncorr[k][i] + 0.01, disorders[i], fontsize=7)
    ax.set_ylabel('node-neighbour correlation')
    ax.set_xlabel('biological rsq')
    ax.set_title(k)
    ax.set_aspect(1.0/ax.get_data_ratio(), adjustable='box')
    plt.savefig(path + 'figures/' + dataset + '/scatter_bio_nn_' + k + '.eps', format='eps')

    plt.ion()
    fig, ax = plt.subplots()
    ax.scatter(conn_rsq, nncorr[k], c=(pspin[k] < 0.05).astype(int))
    for i in range(len(conn_rsq)):
        ax.text(conn_rsq[i] + 0.01, nncorr[k][i] + 0.01, disorders[i], fontsize=7)
    ax.set_ylabel('node-neighbour correlation')
    ax.set_xlabel('connectivity rsq')
    ax.set_title(k)
    ax.set_aspect(1.0/ax.get_data_ratio(), adjustable='box')
    plt.savefig(path + 'figures/' + dataset + '/scatter_conn_nn_' + k + '.eps', format='eps')

"""
Epicentre likelihood
"""

epi = np.zeros(ct.shape)
for i in range(len(disorders)):
    epi[:, i], _ = get_epicentre_likelihood(ct[:, i],
                                            get_neighbour_deformation(ct[:, i],
                                                                      sc, fc))

idx = np.where(pspin['fc-also'] < 0.05)[0]
if 4 and 5 in idx:  # both epilepsies
    epilepsy_mean = np.mean(epi[:, [4, 5]], axis=1).reshape(-1, 1)
    idx = np.delete(idx, (idx == 4) | (idx == 5))
epi_subset = np.concatenate((epi[:, idx], epilepsy_mean), axis=1)

# epicentre likelihood (count)

# thresh = [38, 50, 60]  # 50%, 25%, 10%
epi_summary = np.sum(epi_subset > 38, axis=1)
annot = datasets.fetch_cammoun2012('fsaverage')['scale033']
brains = plotting.plot_fsaverage(data=epi_summary, order='rl',
                                 lhannot=annot.lh, rhannot=annot.rh,
                                 colormap=plt.get_cmap('plasma', np.max(epi_summary)+1),
                                 vmin=np.min(epi_summary),
                                 vmax=np.max(epi_summary),
                                 views=['lat', 'med'],
                                 data_kws={'representation': "wireframe"})
brains.save_image(path+'figures/' + dataset + '/surface_epicentre_likelihood_count.eps')

# epicentre likelihood (mean)
epi_summary = np.mean(epi_subset, axis=1)
annot = datasets.fetch_cammoun2012('fsaverage')['scale033']
brains = plotting.plot_fsaverage(data=epi_summary, order='rl',
                                 lhannot=annot.lh, rhannot=annot.rh,
                                 colormap='plasma',
                                 vmin=np.min(epi_summary),
                                 vmax=np.max(epi_summary),
                                 views=['lat', 'med'],
                                 data_kws={'representation': "wireframe"})
brains.save_image(path+'figures/' + dataset + '/surface_epicentre_likelihood_mean.eps')

# epicentre likelihood (median)
epi_summary = np.median(epi_subset, axis=1)
annot = datasets.fetch_cammoun2012('fsaverage')['scale033']
brains = plotting.plot_fsaverage(data=epi_summary, order='rl',
                                 lhannot=annot.lh, rhannot=annot.rh,
                                 colormap='plasma',
                                 vmin=np.min(epi_summary),
                                 vmax=np.max(epi_summary),
                                 views=['lat', 'med'],
                                 data_kws={'representation': "wireframe"})
brains.save_image(path+'figures/' + dataset + '/surface_epicentre_likelihood_median.eps')
