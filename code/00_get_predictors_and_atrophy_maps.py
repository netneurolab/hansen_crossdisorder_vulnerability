"""
This script creates the variables used repeatedly throughout
the analyses, including:

- matrix of local biological predictors
- matrix of global connectome predictors
- matrix of temporal meg predictors
- region x disorder matrix of Cohen's d effect sizes of case
  vs control cortical thickness from the ENIGMA consortium
"""

import numpy as np
from enigmatoolbox.datasets import load_summary_stats
from scipy.stats import zscore, entropy
from netneurotools import datasets, utils, plotting
import pandas as pd
from bct import degree, centrality, distance, clustering
from scipy.spatial.distance import squareform, pdist
import abagen
from sklearn.decomposition import PCA
import neuromaps
from neuromaps.parcellate import Parcellater
from nilearn.input_data import NiftiLabelsMasker
from nilearn._utils import check_niimg
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import seaborn as sns

def scale_values(values, vmin, vmax, axis=None):
    s = (values - values.min(axis=axis)) / (values.max(axis=axis) - values.min(axis=axis))
    s = s * (vmax - vmin)
    s = s + vmin
    return s

remake_from_scratch = False
make_predictor_surfaces = False
make_disorder_surfaces = False

# set path to directory
path = "C:/Users/justi/OneDrive - McGill University/MisicLab/proj_biology_disease/github/hansen_crossdisorder_vulnerability/"

"""
get parcellation
"""

scale = "scale033"
cammoun = datasets.fetch_cammoun2012()
info = pd.read_csv(cammoun['info'])
cortex = info.query('scale == @scale & structure == "cortex"')['id']
cortex = np.array(cortex) - 1  # python indexing
nnodes = len(cortex)
annot = datasets.fetch_cammoun2012('fsaverage')[scale]

# get distance matrix
coords = utils.get_centroids(cammoun[scale], image_space=True)
eu_distance = squareform(pdist(coords[cortex, :], metric="euclidean"))

# colourmaps
cmap = np.genfromtxt(path+'data/colourmap.csv', delimiter=',')
cmap_div = ListedColormap(cmap)

"""
get the local biological predictors
"""

if remake_from_scratch:
    """
    careful: this code is here more as a reference.
    The t1t2 ratio I use in the analyses is from unrelated HCP subjects only,
    which isn't identical to the t1t2 ratio computed here.
    Also, glycolytic index was shared with me in CIVET space which I then parcellated.
    It's not up on neuromaps because it isn't openly available in its native space
    """

    # gene PC1
    expression = abagen.get_expression_data(cammoun[scale], lr_mirror='bidirectional',
                                            return_donors=True)
    expression = abagen.correct.keep_stable_genes(expression,
                                                  threshold=0.1, percentile=False)
    expression = pd.concat(expression).groupby('label').mean()  # 83 x 14739
    pca = PCA(1)
    gene_pc1 = pca.fit_transform(zscore(expression.iloc[cortex]))
    np.savetxt(path+'data/predictors/gene_pc1.csv', gene_pc1, delimiter=',')

    # receptor PC1
    receptor_data = np.genfromtxt(path+'data/receptors.csv', delimiter=',')
    receptor_pc1 = pca.fit_transform(zscore(receptor_data))
    np.savetxt(path+'data/predictors/receptor_pc1.csv', receptor_pc1, delimiter=',')

    # excitatory-inhibitory ratio
    i_exc = np.array([2, 3, 4, 6, 8, 13, 14])   # 5HT2a, 5HT4, 5HT6, D1, mGluR5, a4b2, M1
    i_inh = np.array([0, 1, 7, 9, 11, 12, 15])  # 5HT1a, 5HT1b, CB1, D2, GABAa, H3, MOR
    receptor_data_scaled = scale_values(receptor_data, 0, 1, axis=0)
    ei_ratio = np.mean(receptor_data_scaled[:, i_exc], axis=1) / \
            np.mean(receptor_data_scaled[:, i_inh], axis=1)
    np.savetxt(path+'data/predictors/ei_ratio.csv', ei_ratio, delimiter=',')

    # receptor entropy
    rnorm = scale_values(receptor_data, 0, 1, axis=0)
    rentropy = entropy(rnorm, axis=1)
    np.savetxt(path+'data/predictors/receptor_entropy.csv', rentropy, delimiter=',')

    # glycolytic index
    # ok sorry I dont' have this in its original space up on neuromaps
    glycolytic_idx = np.genfromtxt(path+'data/predictors/glycolytic.idx', delimiter=',')

    # oxygen metabolism
    cammoun_fslr = datasets.fetch_cammoun2012(version='fslr32k')
    giis = neuromaps.images.relabel_gifti(cammoun_fslr[scale])
    parc = Parcellater(giis, 'fslr')
    cmro2 = neuromaps.datasets.fetch_annotation(desc='cmro2')
    cmro2 = parc.fit_transform(cmro2, 'fslr')
    np.savetxt(path+'data/predictors/oxygen_metabolism.csv', cmro2, deliiter=',')

    # glucose metabolism
    cmrglu = neuromaps.datasets.fetch_annotation(desc='cmruglu')
    cmrglu = parc.fit_transform(cmrglu, 'fslr')
    np.savetxt(path+'data/predictors/glucose_metabolism.csv', cmrglu, deliiter=',')

    # synapse density
    ucbj = path+'data/avg_ucbj_bp_n76_reg.nii.gz'
    mask = NiftiLabelsMasker(cammoun[scale], resampling_target='data')
    img = check_niimg(ucbj, atleast_4d=True)
    ucbj = mask.fit_transform(img).squeeze()
    ucbj = ucbj[cortex]
    np.savetxt(path+'data/predictors/synapse_density.csv', ucbj, delimiter=',')

    # t1t2
    # this grabs all the HCP subjects but the one I use is just
    # the unrelated HCP subjects which is private information soz
    myelin = neuromaps.datasets.fetch_annotation(desc='myelinmap')
    myelin = parc.fit_transform(myelin, 'fslr')
    np.savetxt(path+'data/predictors/t1t2.csv', myelin, delimiter=',')

# aaaand compile

bio_predictor_names = np.array(['gene_pc1', 'receptor_pc1', 'ei_ratio',
                                'glycolytic_idx', 'glucose_metabolism',
                                'synapse_density', 't1t2'])
np.save(path+'data/bio_predictor_names.npy', bio_predictor_names)
predictors = dict([])
for i in range(len(bio_predictor_names)):
    predictors[bio_predictor_names[i]] = np.genfromtxt(path+'data/predictors/'
                                                       + bio_predictor_names[i]
                                                       + '.csv', delimiter=',')

bio = np.zeros([nnodes, len(predictors)])
for i in range(len(predictors)):
    bio[:, i] = predictors[bio_predictor_names[i]]
bio = zscore(bio, axis=0)  # zscore each predictor

np.savetxt(path+'data/local_biol_predictors.csv', bio, delimiter=',')


"""
get the global connectome predictors
"""

rsn_mapping = np.array(info.query('scale == "scale033"')['yeo_7'])

# set to False if using binary SC
weighted_sc = True
# set to True if using fc
weighted_fc = True

if weighted_fc:
    fc = np.load(path+'data/fc_weighted.npy') 
    fc_btw = centrality.betweenness_wei(abs(1/fc)).reshape(-1, 1)
    fc_spd, _ = distance.distance_wei(abs(1/fc))
    fc_spd = np.mean(fc_spd, axis=1).reshape(-1, 1)
    eu = np.mean(eu_distance, axis=1).reshape(-1, 1)
    fc_cc = clustering.clustering_coef_wu(abs(fc)).reshape(-1, 1)
    fc_deg = degree.strengths_und(abs(fc)).reshape(-1, 1)
    # fc_eig = centrality.eigenvector_centrality_und(abs(fc)).reshape(-1, 1)
    fc_pc = centrality.participation_coef(abs(fc),
                                          rsn_mapping[cortex]).reshape(-1, 1)
    fc_mfpt = np.mean(distance.mean_first_passage_time(abs(1/fc)),
                      axis=1).reshape(-1, 1)
    conn_fc = zscore(np.concatenate((fc_deg, fc_btw, fc_spd, eu, fc_pc,
                                     fc_cc, fc_mfpt), axis=1))
    np.savetxt(path+'data/global_conn_predictors_fc.csv', conn_fc, delimiter=',')

if weighted_sc:
    sc = np.load(path+'data/sc_weighted.npy')
    sc_recip = np.zeros((nnodes, nnodes))  # distance transformation
    sc_recip[sc != 0] = 1 / sc[sc != 0]

    btw = centrality.betweenness_wei(sc_recip).reshape(-1, 1)
    spd, _ = distance.distance_wei(sc_recip)
    spd = np.mean(spd, axis=1).reshape(-1, 1)
    cc = clustering.clustering_coef_wu(sc).reshape(-1, 1)
    deg = degree.strengths_und(sc).reshape(-1, 1)

if not(weighted_sc):
    sc = np.load(path+'data/sc_binary.npy')
    btw = centrality.betweenness_bin(sc).reshape(-1, 1)
    spd = np.mean(distance.distance_bin(sc), axis=1).reshape(-1, 1)
    cc = clustering.clustering_coef_bu(sc).reshape(-1, 1)
    deg = degree.degrees_und(sc).reshape(-1, 1)

# eig = centrality.eigenvector_centrality_und(sc).reshape(-1, 1)
eu = np.mean(eu_distance, axis=1).reshape(-1, 1)
pc = centrality.participation_coef(sc, rsn_mapping[cortex]).reshape(-1, 1)
mfpt = np.mean(distance.mean_first_passage_time(sc),
               axis=1).reshape(-1, 1)

conn = zscore(np.concatenate((deg, btw, spd, eu, pc, cc, mfpt),
                             axis=1))
conn_predictor_names = np.array(['deg', 'btw', 'spd', 'eu', 'pc', 'cc', 'mfpt'])
np.save(path+'data/conn_predictor_names.npy', conn_predictor_names)
if weighted_sc:
    np.savetxt(path+'data/global_conn_predictors.csv', conn, delimiter=',')
else:
    np.savetxt(path+'data/global_conn_predictors_bin.csv', conn, delimiter=',')


"""
get the temporal predictors
"""

temp = np.genfromtxt(path+'data/predictors/meg_power.csv', delimiter=',')
temp = zscore(temp, axis=0)
temp_predictor_names = ['delta', 'theta', 'alpha', 'beta', 'gamma1', 'gamma2']
np.save(path+'data/temp_predictor_names.npy', temp_predictor_names)
np.savetxt(path+'data/temporal_predictors.csv', temp, delimiter=',')


"""
get the disorder maps from ENIGMA
"""

## from enigmatoolbox
disorders = ['22q', 'adhd', 'asd', 'epilepsy', 'epilepsy',
                'epilepsy', 'depression', 'ocd', 'schizophrenia', 'bipolar']

spec = ['', '_adult', '_meta_analysis', '_gge',
        '_rtle', '_ltle', '_adult', '_adult', '', '_adult']

ct = np.zeros([nnodes, len(disorders)])
for i in range(len(disorders)):
    sum_stats = load_summary_stats(disorders[i])
    if disorders[i] == 'depression':
        ct[:, i] = sum_stats['CortThick_case_vs_controls{}'
                             .format(spec[i])]['d-icv']
    else:
        ct[:, i] = sum_stats['CortThick_case_vs_controls{}'
                             .format(spec[i])]['d_icv']

# update disorder names
disorders[3] = 'epilepsy_gge'
disorders[4] = 'epilepsy_rtle'
disorders[5] = 'epilepsy_ltle'

## not enigmatoolbox but in same regional order
obesity = np.genfromtxt(path + 'data/enigma/obesity_ct.csv',
                        delimiter=',')
ct = np.concatenate((ct, np.reshape(obesity, (len(obesity), 1))), axis=1)
disorders.append('obesity')

sctpy = pd.read_csv(path+'data/enigma/sctpy_ct.csv')
sctpy_ct = sctpy['Effect_Size_(r)'].to_numpy()
ct = np.concatenate((ct, np.reshape(sctpy_ct, (len(sctpy_ct), 1))), axis=1)
disorders.append('schizotypy')

## convert order to cammoun order
# enigma structure names
enigma_structure = sum_stats['CortThick_case_vs_controls{}'.
                             format(spec[i])]['Structure']                  
for i in range(len(enigma_structure)):
    _, enigma_structure[i] = enigma_structure[i].split('_')

# cammoun structure names
cammoun_structure = info.query('scale == "scale033"')['label']
cammoun_structure = cammoun_structure[cortex]

# match them up
left_cammoun = cammoun_structure[34:]
left_enigma = enigma_structure[:34]
left_reorder = pd.Series(left_enigma.index, index=left_enigma). \
                                                reindex(left_cammoun)
right_cammoun = cammoun_structure[:34]
right_enigma = enigma_structure[34:]
right_reorder = pd.Series(right_enigma.index, index=right_enigma). \
                                                reindex(right_cammoun)

# indeces required to reorder enigma data to cammoun order
reorder = np.concatenate((right_reorder.to_numpy(), left_reorder.to_numpy()))

# reorder the enigma disorders
ct = ct[reorder, :]

## PD is in a totally different order, obviously
parkinsons = pd.read_csv(path+'data/enigma/pd_ct.txt', delimiter=' ')
pd_ct = parkinsons["d"].to_numpy()  # cohen's d
pd_struct = parkinsons['ROIThickness']  # structure names

for i in range(len(pd_struct)):  # make everything lowercase
    pd_struct[i] = pd_struct[i].lower()

# reorder
left_pd = pd_struct[0::2]
left_reorder = pd.Series(left_pd.index, index=left_pd).reindex(left_cammoun)
right_pd = pd_struct[1::2]
right_reorder = pd.Series(right_pd.index, index=right_pd). \
                                            reindex(right_cammoun)

reorder = np.concatenate((right_reorder.to_numpy(), left_reorder.to_numpy()))

pd_ct = pd_ct[reorder]

disorders.append('park')
ct = np.concatenate((ct, np.reshape(pd_ct, (len(pd_ct), 1))), axis=1)  # done

np.save(path+'data/disorders.npy', np.array(disorders))
np.savetxt(path+'data/enigma_ct.csv', ct, delimiter=',')

"""
plot
"""
# plot surfaces
plt.ion()
if make_predictor_surfaces:
    for b in range(len(bio_predictor_names)):
        brain = plotting.plot_fsaverage(data=bio[:, b],
                                        lhannot=annot.lh, rhannot=annot.rh,
                                        order='rl', colormap=cmap_div,
                                        vmin=-max(abs(bio[:, b])),
                                        vmax=max(abs(bio[:, b])),
                                        views=['lat', 'med'],
                                        data_kws={'representation': "wireframe"})
        brain.save_image(path+'figures/png/bio_predictors/surface_' + bio_predictor_names[b] + '.png')
    
    for c in range(len(conn_predictor_names)):
        brain = plotting.plot_fsaverage(data=conn[:, c],
                                        lhannot=annot.lh, rhannot=annot.rh,
                                        order='rl', colormap=cmap_div,
                                        vmin=-max(abs(conn[:, c])),
                                        vmax=max(abs(conn[:, c])),
                                        views=['lat', 'med'],
                                        data_kws={'representation': "wireframe"})
        brain.save_image(path+'figures/png/conn_predictors/surface_' + conn_predictor_names[c] + '.png')

if make_disorder_surfaces:
    for d in range(len(disorders)):
        brain = plotting.plot_fsaverage(data=ct[:, d],
                                        lhannot=annot.lh, rhannot=annot.rh,
                                        order='rl', colormap=cmap_div,
                                        vmin=-np.max(np.abs(ct[:, d])),
                                        vmax=np.max(np.abs(ct[:, d])),
                                        views=['lat', 'med'],
                                        data_kws={'representation': "wireframe"})
        brain.save_image(path+'figures/png/disorders_v2/surface_' + disorders[d] + '.png')

# plot heatmaps
plt.rcParams['eps.fonttype'] = 'none'
plt.rcParams['font.size'] = 8.0

plt.ion()
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
sns.heatmap(np.corrcoef(bio.T), annot=True, square=True,
            mask=np.tril(np.ones(bio.shape[1])),
            cmap=cmap_div, vmin=-1, vmax=1, cbar=False,
            ax=ax1, linewidths=.5,
            xticklabels=bio_predictor_names,
            yticklabels=bio_predictor_names)
sns.heatmap(np.corrcoef(conn.T), annot=True, square=True,
            mask=np.tril(np.ones(conn.shape[1])),
            cmap=cmap_div, vmin=-1, vmax=1, cbar=False,
            ax=ax2, linewidths=.5,
            xticklabels=conn_predictor_names,
            yticklabels=conn_predictor_names)
plt.tight_layout()
plt.savefig(path+'figures/heatmap_predictor_corrs.svg', format='svg')