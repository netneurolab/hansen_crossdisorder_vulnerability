"""
This script compares cross-disorder "similarity" to various other annotations.
These analyses correspond to Figure 4 in the manuscript.
"""

import numpy as np
import pandas as pd
from scipy.stats import zscore, pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from netneurotools import stats, datasets, utils
from scipy.spatial.distance import squareform, pdist
from scipy.optimize import curve_fit
from matplotlib.colors import ListedColormap
import abagen
from sklearn.utils.validation import check_random_state


def exponential(x, a, b, c):
    return a*np.exp(b*x)+c


def regress_dist(x, eu_distance, pars):
    return x - exponential(eu_distance, pars[0], pars[1], pars[2])


def match_length_degree_distribution(data, eu_distance, nbins=10, nswap=None, seed=None):
    """
    Takes a weighted, symmetric connectivity matrix `data` and Euclidean/fiber 
    length matrix `distance` and generates a randomized network with:
        1. exactly the same degree sequence
        2. approximately the same edge length distribution
        3. exactly the same edge weight distribution
        4. approximately the same weight-length relationship

    Parameters
    ----------
    data : (N, N) array-like
        weighted or binary symmetric connectivity matrix.
    distance : (N, N) array-like
        symmetric distance matrix.
    nbins : int
        number of distance bins (edge length matrix is performed by swapping
        connections in the same bin). Default = 10.
    nswap : int
        total number of edge swaps to perform. Recommended = nnodes * 20.
        Default = None.

    Returns
    -------
    data : (N, N) array-like
        binary rewired matrix
    W : (N, N) array-like
        weighted rewired matrix
        
    Reference
    ---------
    Betzel, R. F., Bassett, D. S. (2018) Specificity and robustness of long-distance
    connections in weighted, interareal connectomes. PNAS.

    """
    rs = check_random_state(seed)

    nnodes = len(data)             # number of nodes
    
    if nswap is None:
        nswap = nnodes*20          # set default number of swaps
    
    mask = data != 0               # nonzero elements
    mask = np.triu(mask, 1)        # keep upper triangle only
    weights = data[mask]           # values of edge weights
    distances = eu_distance[mask]  # values of edge lengths
    Jdx = np.argsort(distances)    # indices to sort distances in ascending order
    
    bins = np.linspace(min(eu_distance[eu_distance != 0]),
                       max(eu_distance[eu_distance != 0]),
                       nbins+1)  # length/distance of bins
    bins[-1] += 1
    B = np.zeros((nnodes, nnodes, nbins))  # initiate 3D stack of bins
    for k in range(nbins):
        # element is k+1 if the distance falls within the bin, 0 otherwise
        B[:, :, k] = np.logical_and(eu_distance >= bins[k],
                                   eu_distance < bins[k + 1]) * (k + 1)
    # matrix of distance bins
    Bsum = np.sum(B, axis=2)
    
    tmp = np.triu((data != 0)*Bsum, 1)
    row_idx, col_idx = tmp.nonzero()  # indices of edges
    vals = tmp[row_idx, col_idx]
    nedges = len(row_idx)  # number of edges
    iswap = 0             # swap counter
    
    while iswap < nswap:
        myEdge = rs.randint(nedges)   # get a random edge index
        myEdge_row = row_idx[myEdge]  # row idx of edge
        myEdge_col = col_idx[myEdge]  # col idx of edge
        myEdge_bin = vals[myEdge]     # bin of edge
        
        # get indices that can be swapped
        indkeep = (row_idx != myEdge_row) & (row_idx != myEdge_col) \
                  & (col_idx != myEdge_row) & (col_idx != myEdge_col)

        row_idx_keep = row_idx[indkeep]
        col_idx_keep = col_idx[indkeep]
        
        bins_keep = vals[indkeep]  # bins of possible swaps
        
        edge_row = myEdge_row*nnodes + row_idx_keep # edge indices
        edge_row_bins = Bsum[np.unravel_index(edge_row, Bsum.shape)] # matlab-style linear indexing
        edge_col = myEdge_col*nnodes + col_idx_keep # other set of edge indices
        edge_col_bins = Bsum[np.unravel_index(edge_col, Bsum.shape)] 
        
        # get good list of indices
        idx1 = np.logical_and(myEdge_bin == edge_row_bins,
                              bins_keep == edge_col_bins)
        # get other set of good indices
        idx2 = np.logical_and(myEdge_bin == edge_col_bins,
                              bins_keep == edge_row_bins)
        # full set
        goodidx = np.logical_or(idx1, idx2)
        
        # update the indices to keep
        row_idx_keep = row_idx_keep[goodidx]
        col_idx_keep = col_idx_keep[goodidx]
        
        # update the edge indices
        edge_row = myEdge_row*nnodes + row_idx_keep
        edge_col = myEdge_col*nnodes + col_idx_keep
        
        data_row = data[np.unravel_index(edge_row, data.shape)]
        data_col = data[np.unravel_index(edge_col, data.shape)]
        
        # find missing edges
        ind = np.where(np.logical_and(data_row == 0,
                                      data_col == 0).astype(int))[0]
        
        if len(ind) > 0:  # if there is a missing edge
            
            # choose a random swap
            random_swap = ind[rs.randint(len(ind))]
            
            # do the swap
            row_idx_keep = row_idx_keep[random_swap]
            col_idx_keep = col_idx_keep[random_swap]
            
            data[myEdge_row, myEdge_col] = 0
            data[myEdge_col, myEdge_row] = 0
            
            data[row_idx_keep, col_idx_keep] = 0
            data[col_idx_keep, row_idx_keep] = 0
            
            data[myEdge_row, row_idx_keep] = 1
            data[row_idx_keep, myEdge_row] = 1
            
            data[myEdge_col, col_idx_keep] = 1
            data[col_idx_keep, myEdge_col] = 1
            
            other_edge = np.where(indkeep)[0]
            other_edge = other_edge[goodidx]
            other_edge = other_edge[random_swap]
            
            row_idx[myEdge] = min(myEdge_row, row_idx_keep)
            col_idx[myEdge] = max(myEdge_row, row_idx_keep)
            
            row_idx[other_edge] = min(myEdge_col, col_idx_keep)
            col_idx[other_edge] = max(myEdge_col, col_idx_keep)
            
            vals[myEdge] = Bsum[myEdge_row, row_idx_keep]
            vals[other_edge] = Bsum[myEdge_col, col_idx_keep]
            
            iswap += 1
            if iswap % 100 == 0:
                print(iswap)
    
    d = eu_distance[np.where(np.triu(data, 1))]  # get distances where edges are
    jdx = np.argsort(d)                      # sort distances (ascending)
    W = np.zeros((nnodes, nnodes))           # output matrix
    # add weights
    W[np.where(np.triu(data,1))[0][jdx],
      np.where(np.triu(data,1))[1][jdx]] = weights[Jdx]
    
    return data, W


def corr_spin(x, y, spins, nspins):
    rho, _ = pearsonr(x, y)
    null = np.zeros((nspins,))
        
    if len(x) == spins.shape[0] - 1:  # if insula is missing
        np.append(x, np.nan)
        np.append(y, np.nan)
        
    # null correlation
    for i in range(nspins):
        tmp = y[spins[:, i]]
        # convert to dataframe for better handling of nans
        # in the case that insula is missing
        df = pd.DataFrame(np.stack((x, tmp), axis=1), columns=['x', 'y'])
        null[i] = df["x"].corr(df["y"])

    pval = (1 + sum(abs((null - np.mean(null))) >
                    abs((rho - np.mean(null))))) / (nspins + 1)
    return rho, pval


# set path to directory
path = "C:/Users/justi/OneDrive - McGill University/MisicLab/proj_biology_disease/github/hansen_crossdisorder_vulnerability/"

"""
load
"""
# disorder maps
ct = np.genfromtxt(path+'data/enigma_ct.csv', delimiter=',')
ct = zscore(ct)
disorders = np.load(path+'data/disorders.npy')

# predictors
bio = np.genfromtxt(path+'data/local_biol_predictors.csv', delimiter=',')
conn = np.genfromtxt(path+'data/global_conn_predictors.csv', delimiter=',')

# sc and fc
sc = np.load(path+'data/sc_binary.npy')
fc = np.load(path+'data/fc_weighted.npy')

# parcellation, spin samples, rsn labels, 
cammoun = datasets.fetch_cammoun2012()
info = pd.read_csv(cammoun['info'])
cortex = info.query('scale == "scale033" & structure == "cortex"')['id']
cortex = np.array(cortex) - 1  # python indexing
nnodes = len(cortex)
hemiid = np.array(info.query('scale == "scale033"')['hemisphere'])
hemiid = hemiid == 'R'
coords = utils.get_centroids(cammoun['scale033'], image_space=True)
coords = coords[cortex, :]
nspins = 10000
spins = stats.gen_spinsamples(coords, hemiid[cortex], n_rotate=nspins, seed=1234)
rsn_mapping = np.array(info.query('scale == "scale033"')['yeo_7'])

# gene coexpression - takes a bit
expression = abagen.get_expression_data(cammoun['scale033'], return_donors=True)
expression, ds = abagen.correct.keep_stable_genes(expression,
                                                  threshold=0.1,
                                                  percentile=False,
                                                  return_stability=True)
expression = pd.concat(expression).groupby('label').mean()
expression = zscore(expression.iloc[cortex[34:]])  # left hemisphere

# receptor density
receptors = np.genfromtxt(path+'data/receptors.csv', delimiter=',')

# colourmap
cmap = np.genfromtxt(path+'data/colourmap.csv', delimiter=',')
cmap_div = ListedColormap(cmap)
cmap_seq = ListedColormap(cmap[128:, :])
cmap_blue = ListedColormap(np.flip(cmap[:128, :], axis=0))

"""
make similarity matrices
"""

# upper triangular mask for plotting
mask = np.triu(np.ones(nnodes), 1) > 0
maskleft = np.triu(np.ones(int(nnodes/2)), 1) > 0

# similarity matrices
dis_sim = np.corrcoef(zscore(ct))
bio_sim = np.corrcoef(bio)
conn_sim = np.corrcoef(conn) 
gen_sim = np.corrcoef(expression)  # just left hem
rec_sim = np.corrcoef(zscore(receptors))

sim_mats = {"disorder" : dis_sim,
            "biological" : bio_sim,
            "connectivity" : conn_sim,
            "receptor" : rec_sim,
            "functional" : fc}

# distance matrix
eu_distance = squareform(pdist(coords, metric = "euclidean"))

"""
regress out distance
"""

p0 = [1, -0.05, -0.1]  # initial parameter guesses

pars = {}  # exponential curve variables
sim_mats_reg = {}  # distance-regressed similarity matrices (upper triangular)

for key in sim_mats.keys():
    pars[key], _ = curve_fit(exponential, eu_distance[mask], sim_mats[key][mask], p0)
    sim_mats_reg[key] = regress_dist(sim_mats[key][mask], eu_distance[mask], pars[key])

# do it again for gene coexp (because left hem only)
pars['gene'], _ = curve_fit(exponential, eu_distance[34:, 34:][maskleft],
                    gen_sim[maskleft], p0=p0)
gen_sim_reg = regress_dist(gen_sim[maskleft], eu_distance[34:, 34:][maskleft],
                           pars['gene'])

# need left hemisphere in matrix form for gene exp bah
dis_sim_reg_mat = np.zeros((nnodes, nnodes))
dis_sim_reg_mat[np.triu(np.ones(nnodes, dtype=bool), 1)] = sim_mats_reg['disorder']
dis_sim_reg_left = dis_sim_reg_mat[34:, 34:][maskleft]

## get pvals
rho = {}
pval = {}
for key in sim_mats.keys():
    rho[key], pval[key] = pearsonr(sim_mats_reg['disorder'],
                                   sim_mats_reg[key])
rho['gene'], pval['gene'] = pearsonr(dis_sim_reg_left,
                                      gen_sim_reg)
sim_mats_reg['gene'] = gen_sim_reg

"""
plot
"""
# disorder similarity matrix
plt.ion()
plt.figure()
sns.heatmap(dis_sim, vmin=-1, vmax=1, cmap=cmap_div)
plt.savefig(path+'figures/heatmap_disorder_similarity.eps', format='eps')

# histogram
plt.ion()
plt.figure()
ax = sns.distplot(dis_sim[mask])
ax.set(xlabel = 'disorder similarity')
plt.savefig(path+'figures/hist_disorder_similarity.eps', format='eps')

# scatter plots
sim_label = ['disorders similarity', 'local biological similarity',
          'global connectivity similarity', 'neurotransmitter receptor similarity',
          'functional connectivity', 'gene coexpression']

fig, axs = plt.subplots(3, 2, figsize=(6, 7))  # distance regressed
axs = axs.ravel()
counter = 0

for key in sim_mats_reg.keys():
    if key == 'disorder':
        continue
    if key == 'gene':
        axs[counter].scatter(dis_sim_reg_left, sim_mats_reg[key], s=2)
    else:
        axs[counter].scatter(sim_mats_reg['disorder'], sim_mats_reg[key], s=2)
    axs[counter].set_xlabel('disorder similarity')
    axs[counter].set_ylabel(sim_label[counter + 1])
    axs[counter].set_title('rho = ' + str(rho[key])[:4]
                    + ', pspin = ' + str(pval[key])[:5])
    counter += 1
plt.tight_layout()
plt.savefig(path+'figures/scatter_dissim_distance_reg.eps', format='eps')

"""
# the other way of making this figure lol
fig, axs = plt.subplots(3, 2, figsize=(6, 7))  # not distance regressed
axs[0, 0].scatter(dis_sim[mask], bio_sim[mask], s=2)
axs[0, 0].set_xlabel('disorder similarity')
axs[0, 0].set_ylabel('biological similarity')
axs[0, 1].scatter(dis_sim[mask], conn_sim[mask], s=2)
axs[0, 1].set_xlabel('disorder similarity')
axs[0, 1].set_ylabel('connectivity similarity')
axs[1, 0].scatter(dis_sim[mask], rec_sim[mask], s=2)
axs[1, 0].set_xlabel('disorder similarity')
axs[1, 0].set_ylabel('receptor similarity')
axs[1, 1].scatter(dis_sim[34:, 34:][maskleft], gen_sim[maskleft], s=2)
axs[1, 1].set_xlabel('disorder similarity')
axs[1, 1].set_ylabel('gene coexpression')
axs[2, 1].scatter(dis_sim[mask], fc[mask], s=2)
axs[2, 1].set_xlabel('disorder similarity')
axs[2, 1].set_ylabel('functional connectivity')
plt.tight_layout()
plt.savefig(path+'figures/scatter_dissim_raw.eps', format='eps')
"""

"""
relationship with SC and FC
"""

## connected vs not connected

# set up dictionary with connected vs not connected receptor similarity
d = dict({'connected': dis_sim[mask][np.where(sc[mask] == 1)],
          'not connected': dis_sim[mask][np.where(sc[mask] == 0)]})
df_sc = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in d.items()]))

emp = np.mean(df_sc['connected']) - np.mean(df_sc['not connected'])

null = np.zeros((nspins, 1))
for i in range(nspins):
    sc_rewired, _ = match_length_degree_distribution(sc,
                                                     eu_distance,
                                                     10, nnodes*20)
    null[i] = np.mean(dis_sim[mask]
                      [np.where(sc_rewired[mask] == 1)]) \
        - np.mean(dis_sim[mask]
                  [np.where(sc_rewired[mask] == 0)])

pval_sc = (1 + np.sum(np.abs((null - np.mean(null)))
                   >= abs((emp - np.mean(null))))) / (nspins + 1)
np.save(path+'results/null_edge-degree-preserving.npy', null)

## within vs between

# is an edge within or between networks?
withbet = np.zeros(fc.shape)
for k in range(nnodes):
    for j in range(nnodes):
        if rsn_mapping[cortex[k]] == rsn_mapping[cortex[j]]:
            withbet[k, j] = 1

# dictionary of within vs between receptor similarity
d = dict({'within': dis_sim[mask][np.where(withbet[mask] == 1)],
          'between': dis_sim[mask][np.where(withbet[mask] == 0)]})
df_fc = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in d.items()]))

emp = np.mean(df_fc['within']) - np.mean(df_fc['between'])
null = np.zeros([nspins, 1])
for i in range(nspins):
    rsn_null = rsn_mapping[cortex[spins[:, i]]]
    withbet_null = np.zeros(fc.shape)
    for k in range(nnodes):
        for j in range(nnodes):
            if rsn_null[k] == rsn_mapping[cortex][j]:
                withbet_null[k, j] = 1
    fc_null = dict({'within': dis_sim[mask]
                   [np.where(withbet_null[mask] == 1)],
                   'between': dis_sim[mask]
                   [np.where(withbet_null[mask] == 0)]})
    null[i] = np.mean(fc_null['within']) - np.mean(fc_null['between'])

pval_fc = (1 + np.sum(np.abs((null - np.mean(null)))
                   >= abs((emp - np.mean(null))))) / (nspins + 1)

# plot
fig, (ax1, ax2) = plt.subplots(1, 2)
sns.boxplot(data=df_sc, ax=ax1)
ax1.set_ylabel('disorder similarity')
ax1.set_title(['p = ' + str(pval_sc)[:5]])
sns.boxplot(data=df_fc, ax=ax2)
ax2.set_title(['pspin = ' + str(pval_fc)[:5]])
plt.tight_layout()
plt.savefig(path+'figures/boxplot_sc_fc.eps', format='eps')


"""
leave-one-out
"""

loo_rho = np.zeros((len(disorders), ))
for k in range(len(disorders)):
    # disorder similarity when you remove one disorder
    tmp = np.corrcoef(np.delete(ct, k, axis=1))
    # correlate with complete disorder similarity matrix
    loo_rho[k], _ = pearsonr(tmp[mask], dis_sim[mask])

plt.ion()
ax = sns.violinplot(data=loo_rho)
ax.set(ylabel='correlation')
plt.savefig(path+'figures/violin_loo.eps', format='eps')

# disorder influence
plt.ion()
plt.bar(range(len(disorders)), 1 - loo_rho)
plt.xticks(range(len(disorders)), labels=disorders, rotation='vertical')
plt.tight_layout()
plt.savefig(path+'figures/bar_loo.eps', format='eps')


"""
supplementary exponential relationships
"""

x = [eu_distance[mask]] * 5
x.append(eu_distance[34:, 34:][maskleft])
y = [dis_sim[mask], bio_sim[mask], conn_sim[mask], rec_sim[mask],
     fc[mask], gen_sim[maskleft]]
pars_label = ['disorder', 'biological', 'connectivity', 'receptor',
              'functional', 'gene']


plt.ion()
fig, axs = plt.subplots(2, 3, figsize=(10, 5.5))
axs = axs.ravel()
for i in range(len(x)):
    axs[i].scatter(x[i], y[i], s=5, c='#faa842', edgecolors='none')
    axs[i].plot(np.arange(10, 160, 1),
                exponential(np.arange(10, 160, 1),
                            pars[pars_label[i]][0],
                            pars[pars_label[i]][1],
                            pars[pars_label[i]][2]), c='r')
    axs[i].set_xlabel('Euclidean distance')
    axs[i].set_ylabel(sim_label[i])
    axs[i].set_title('y = ' + str(pars[pars_label[i]][0])[:4]
                     + ' * exp(' + str(pars[pars_label[i]][1])[:5]
                     + ' * x) + '+ str(pars[pars_label[i]][2])[:5])
plt.tight_layout()
plt.savefig(path+'figures/scatter_exponentials.eps', format='eps')
